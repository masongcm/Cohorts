
\section{Data \label{sec:data}}

We use information from two nationally representative longitudinal studies in the UK. The studies follow the lives of children born approximately 30 years apart: the British Cohort Study (BCS) surveys individuals born in 1970, and the Millennium Cohort Study (MCS) includes births between 2000 and 2002. The British Cohort Study includes all individuals born across Great Britain in a single week (April 4-11, 1970). Cohort members' families -- and subsequently members themselves -- were surveyed on multiple occasions. For this paper we augment the information at the five-year survey with data from birth, adolescence (16), and adulthood (30, 38, 42). The Millennium Cohort Study follows individuals born in the UK between September 2000 and January 2002. We use the birth survey, and the sweeps at around 5, 11, and 14 years.\footnote{All data is publicly available at the UK Data Service \citep{Chamberlain2013,Butler2016a,Butler2016b,Butler2017,UniversityOfLondon.InstituteOfEducation.CentreForLongitudinalStudies2016b,UniversityOfLondon.InstituteOfEducation.CentreForLongitudinalStudies2016,UniversityOfLondon.InstituteOfEducation.CentreForLongitudinalStudies2016a,UniversityOfLondon.InstituteOfEducation.CentreForLongitudinalStudies2017b,UniversityOfLondon.InstituteOfEducation.CentreForLongitudinalStudies2017,UniversityOfLondon.InstituteOfEducation.CentreForLongitudinalStudies2017a,UniversityOfLondon.InstituteOfEducation.CentreForLongitudinalStudies2017c}.}

Our main focus is on socio-emotional skills of children around age five. We take advantage of the longitudinal nature of the cohorts by merging information from surveys before and after age five. From the birth survey, we include information on gestational age and weight at birth, previous stillbirths, parity, maternal smoking in pregnancy, maternal age, height, and marital status. From the five year survey, we extract maternal education, employment status, and the father's occupation. All the above variables are transformed or recoded to maximise comparability between the two studies. Furthermore, we add some adolescent risky behaviours such as smoking and BMI, with the caveat that these are surveyed at 16 and 14 in BCS and MCS respectively. Finally, for the 1970 cohort we also include measures of adult educational attainment, BMI, and income. Variable definitions are available in \autoref{tab:harmvar}.

Ideally, we would compare socio-emotional skills alongside cognitive skills. However, the cognitive tests administered to each cohort have no overlap, even at the item level. We thus use the available cognitive tests in each cohort to estimate simple confirmatory factor model with a single latent dimension, separately by cohort (see \autoref{tab:harmvar} for the tests used). Unlike the other indicators in our analysis, cognitive skills are thus not comparable across cohort. 

Another complication arises from the fact that, differently from the British Cohort Study, the Millennium Cohort Study has a stratified design. It oversamples children living in administrative areas characterised by higher socioeconomic deprivation and larger ethnic minority population \citep{Plewis2007}. We rebalance the MCS sample to make it nationally representative by excluding from the analysis a fraction of observations from the oversampled areas, proportionally to their sampling probability.\footnote{See Table 5.5 in \citet{Plewis2007}. This choice is mainly driven by software limitations. The \texttt{lavaan} package in \textbf{\textsf{R}} \citep{Rosseel2012} is the most suitable tool for our invariance analysis, but it does not allow to use weights when outcomes are categorical, as it is the case for the socio-emotional measurements.}
We also restrict our sample to individuals born in England. Finally, we restrict the sample to cases where the respondent in the five-year followup was the natural mother, and where there is complete information on socioemotional skills. The final sample contains 9,545 individuals from the British Cohort Study, and 5,436 from the Millennium Cohort Study.

\section{Dimensions of  socio-emotional skills \label{sec:methods}}

Child socio-emotional skills are an unobservable and difficult to measure construct. Over recent years, the measurement of such skills has evolved and, over time, different measures have been used.  As we discuss below, this makes the comparison  of socio emotional skills across different groups, assessed with different tools, difficult.

A common approach to infer a child's socio-emotional development is based on behavioural screening scales. As part of these tools, mothers (or teachers) indicate whether their children exhibit a series of behaviours -- the \emph{items} of the scale. In the British and Millennium Cohort Studies, two different scales were employed. In the BCS, the Rutter A Scale was used \citep{Rutter1970} while in the MCS cohort, mothers were  administered the Strengths and Difficulties Questionnaire (SDQ, \citealp{Goodman1994,Goodman1997}). The SDQ was created as an update to the Rutter scale. It encompasses more recent advances in child psychopathology, and emphasises positive traits alongside undesirable ones \citep{Stone2010}. The Rutter and SDQ scales are reproduced in \autoref{tab:scales}; they have 23 and 25 items each, respectively. In the child psychiatry and psychology literatures, the Rutter and SDQ behavioural screening scales are regarded as measures of behavioural problems and mental health. However, in our analysis we follow the economics literature, and - after having recoded them accordingly - we interpret them as measures of positive child development \citep{Goodman2011}.

While the Rutter and SDQ scales are similar in their components, there is no a priori reason to expect them to be directly comparable. First, the overlap of behaviours described in the two scales is only partial. Second, the wording of each item is slightly different, both in the description and in the options that can be selected as answers. Third, the different ordering of the items within each scale might lead to order effects. Fourth, and no less importantly, the interpretation of each behaviour by respondents living 30 years apart (1975 vs 2005/6) might differ due to a host of evolving societal norms.

As our goal is to compare socio-emotional skills across the two cohorts, we construct a new scale by retaining the items that are worded in a similar way across the two original Rutter and SDQ scales, and making some slight coding adjustments to maximise comparability. In what follows, we will consider the included items to be the same \emph{measure} in the two cohorts. The wording of the items we will be using in the analysis is presented in \autoref{tab:scalecomp}: we retain 13 items for the BCS (two of them are grouped) and 11 for the MCS with high degree of comparability.\footnote{We exclude from the analysis items that were completely different between the two questionnaires, although we could have included them in the factor analysis and treated them as missing in the cohort were they were not administered. While this could have improved efficiency, we decided to rely on a more coherent set of measures to maximise comparability between the two cohorts.}. Item-level prevalence by cohort and gender is in \autoref{tab:meantab}. We see that, in general, item prevalence is more similar across genders within the same cohort, than across cohorts. For the majority of items, there is a lower prevalence of problematic behaviours in the MCS than in the BCS; however, four items (distracted, tantrums, fearful, aches) show a higher prevalence in 2005 than in 1975. Regardless, a simple cross-cohort comparison of item-level prevalence is misleading because of changing perceptions and norms about what constitutes problematic behaviour in children. The analysis in section \ref{sec:measinv} tackles this issue.

In the remainder of this section, we analyse the properties of the new scale. In particular, we study the \emph{factor structure} of our scale. Namely, we establish how many latent dimensions of socio-emotional skills the scale is capturing, and which items are measuring which dimension. We then estimate the parameters of the factor models that corresponds to our choice of dimension and attribution of specific items to factors. In the following section, we investigate to what extent socio-emotional skills are measured in the same way across cohorts.

\subsection{Exploratory analysis \label{sec:ea}}

The original Rutter scale, used for the BCS cohort, distinguishes behaviours into two subscales: \emph{anti-social} and \emph{neurotic} \citep{Rutter1970}. This two-factor conceptualisation has been validated using data from multiple contexts, and the latent dimensions have been broadly identified as externalising and internalising behaviour problems.\footnote{See for example \citet{Fowler1979,Venables1983,Tremblay1987,Berglund1999,Klein2009}. However, in some cases a three-factor structure was found to better fit the data, with the externalising factor separating into two factors seemingly capturing aggressive and hyperactive behaviours \citep{Behar1974,McGee1985}.} The Strength and Difficulties Questionnaire, used for the MCS cohort, was conceived to have five subscales of five items each. The five subscales are: \emph{hyperactivity}, \emph{emotional symptoms}, \emph{conduct problems}, \emph{peer problems}, and \emph{prosocial}. This five-factor structure has been validated in many contexts \citep{Stone2010}; lower-dimensional structures have been also suggested \citep{Dickey2004}. Recent research has shown that there are some benefits to using broader subscales that correspond to the externalising and internalising factors in Rutter, especially in low-risk or general population samples \citep{Goodman2010}.

We use exploratory factor analysis (EFA) to assess the factors structure of our 11-item scale combining Rutter and SDQ.\footnote{Factor-analytic methods have long been used in psychology, and in recent years they have become increasingly popular in economics, especially to meaningfully aggregate high-dimensional items measuring different aspects of common underlying dimensions of human development. The EFA is performed decomposing the polychoric correlation matrix of the items and using weighted least squares, and the solution is rescaled using oblique factor rotation (\emph{oblimin}). We use the \textbf{\textsf{R}} package \texttt{psych}, version 1.8.4 \citep{Revelle2018}.} We start by investigating the number of latent constructs that are captured by the scale, using different methods developed in the psychometric literature, and now also used in the economics literature. The results are displayed in \autoref{tab:numfac}. As pointed out in \citet{Conti2014}, there is relatively little agreement among procedures; this is the case especially for the Rutter items in the BCS data, where different methods suggest to retain between 1 and 3 factors, while most methods suggest to retain 2 factors in the MCS. In our analysis, we adopt two factors and a dedicated measurement system, where each measure reflects only one factor. This choice is justified both by the child psychology literature cited above, and as compromise to work with the same number of factors in the two cohorts. The two-factor EFA delivers a neat and sensible separation between items, as shown in \autoref{tab:load}: reassuringly, similarly-worded items load on the same factor across the two cohorts, and also the magnitude of the respective loadings (measuring the strength of the association between the item and the factor) is very smilar. Following previous research, we name the first dimension \emph{Externalising skills} (EXT, comprising the items restless, squirmy/fidgety, fights/bullies, distracted, tantrums, and disobedient) and the second dimension \emph{Internalising skills} (INT, comprising the items worried, fearful, solitary, unhappy, and aches).

\subsection{Factor model \label{sec:fm}}

Equipped with the factor structure inferred in the previous section, we specify a multiple-group factor analysis model to formally quantify the strength of the relationship between the observed items in our scale and the two latent socio-emotional skills. We specify two groups of children $c=\{BCS,MCS\}$, corresponding to the two cohorts. Each individual child is denoted by $j=1\dots N_c$, where $N_c$ is the number of children in cohort $c$. For each child $j$ in cohort $c$, we observe categorical items $X_{ijc}$ with $i=1,\dots,11$, corresponding to the eleven maternal reports in \autoref{tab:scalecomp}. We assume that each child is characterised by a latent bi-dimensional vector of externalising and internalising socio-emotional skills $\bm{\theta}_{jc} = (\theta_{jc}^{EXT},\theta_{jc}^{INT})$, as shown by the EFA in the previous section.

Children are assumed to have a latent continuous propensity $X^*_{ijc}$ for each item $i=1,\dots,I$. We model this propensity as a function of item- and cohort-specific intercepts $\nu_{ic}$ and loadings $\bm{\lambda}_{ic}$, and the child's latent skills $\bm{\theta}_{jc}$, plus an independent error component $u_{ijc}$. The propensity for each item can be written as follows:

$$X^*_{ijc} =\nu_{ic} + \bm{\lambda}_{ic}\bm{\theta}_{jc} + u_{ijc} \qquad \text{for} \quad i = 1,\dots,11$$

or more compactly:
\be\label{eq:propens}
\bm{X}^*_{jc} =\bm{\nu}_c + \bm{\Lambda}_{c}\bm{\theta}_{jc} + \bm{u}_{jc}
\ee

We make the common assumption of a dedicated (or congeneric) factor structure, where each measure is assumed to load on only one latent dimension \citep{Heckman2013,Conti2010a,Attanasio2018a}. We mirror the structure found in the exploratory factor analysis (see \autoref{tab:load}), and assume that items 1-6 load exclusively on the externalising factor and items 7-11 on the internalising factor.\footnote{The dedicated factor structure corresponds to a sparse loading matrix, i.e.: $$\bm{\Lambda}_{c} \coloneqq \begin{bmatrix} \lambda_{1c}, \dots, \lambda_{6c} & \bm{0} \\ \bm{0} & \lambda_{7c}, \dots, \lambda_{11c} \end{bmatrix}.$$}

The discrete ordered nature of the observed measures $X_{ijc}$ is incorporated by introducing item- and cohort-specific threshold parameters $\tau_{ic}$ \citep{Muthen1984}. The observed measures as a function of the propensities $X^*$ can be then written as follows:
\be\label{eq:thresh}
X_{ijc} = s \qquad \text{if} \; \tau_{s,ic} \leq X^*_{ijc} < \tau_{s+1,ic} \qquad \text{for} \; s=0,1,2
\ee
with $\tau_{0,ic}=-\infty$ and $\tau_{3,ic}=+\infty$. Notice that we recode all ordered items to have higher values for \emph{better} behaviours, so that our latent vectors can be interpreted as favourable skills and not behavioural problems.

The model implies the following expression for the mean and covariance structure of the latent propensities:
$$\bm{\mu}_c=\bm{\nu}_c + \bm{\Lambda}_c\bm{\kappa}_c \qquad \text{and} \qquad \Sigma_c = \bm{\Lambda}_c \bm{\Phi}_c \bm{\Lambda}_c^\prime + \bm{\Psi}_c.$$

The model restrictions in \eqref{eq:propens} and \eqref{eq:thresh} do not identify the parameters without additional assumptions. As per the traditional factor analysis approach, we impose a normal distribution on the latent skills and error terms:\footnote{Recent work has also used mixtures of normals for the latent factors distribution, e.g. \citet{Conti2010a}.}
\be\label{eq:norm}
\bm{\theta}_{jc} \sim N(\bm{\kappa}_c, \bm{\Phi}_c) \qquad \text{and} \qquad \bm{u}_{jc} \sim N(\bm{0}, \bm{\Psi}_c).
\ee

Even with these assumptions, there are infinite equivalent parameterisations through which the model can be identified -- the well-known issue of factor indeterminacy. We follow common practice and identify the model by setting the mean $\bm{\kappa}$ and variance $\bm{\Phi}$ of the latent skill factor in both cohorts to zero and one, respectively. Furthermore, we set intercepts to zero and error variances to one. Loadings $\lambda$ and thresholds $\tau$ are instead allowed to vary across cohorts.

\be\label{eq:thetapar}
\text{diag}(\bm{\Phi}_c) = \bm{I}, \quad \bm{\kappa}_c=\bm{0}, \quad \bm{\nu}_c=\bm{0}, \quad \text{and} \; \text{diag}(\bm{\Psi}_c) = \bm{I} \qquad \forall c \in \{BCS,MCS\}.
\ee

The restrictions in \eqref{eq:propens}, \eqref{eq:thresh}, \eqref{eq:norm}, and \eqref{eq:thetapar} define the so-called \emph{configural} model. This is a `minimum' identifiable model, in that it places the least possible restrictions on how parameters are allowed to vary across cohorts. It serves as a basis for our measurement invariance analysis in the next section.\footnote{This set of identifying restrictions is known as \emph{Theta} parameterisation \citep{Wu2016a}. See \aref{asec:midet} for statistically equivalent alternative parameterisations.}

\section{Measurement invariance \label{sec:measinv}}

Any comparison between socioemotional skills across the two cohorts requires that the measures at our disposal have the same relationship with the latent constructs of interest in both cohorts. In other words, the items in our new scale must measure externalising and internalising socioemotional skills in the same way in the BCS and MCS data. This property is denominated measurement invariance \citep{Vandenberg2000a,Putnick2016}.

In the framework of factor analysis, measurement invariance is a formally testable property. In this paper, we follow the recent identification methodology by \citet{Wu2016a}. The configural model defined in the previous section serves as the starting point. Measurement invariance is then assessed by comparing the configural model to a series of hierarchically nested models. These models place increasing restrictions on the item parameters, constraining them to be equal across groups. Their fit is then compared to that of the configural model. Intuitively, if the additional cross-group restrictions have not significantly worsened model fit, one can conclude that a certain level of invariance is achieved. The hierarchy of restrictions is detailed in \autoref{tab:invparam}.

Let's consider examples from our application. A \emph{loading and threshold invariance} model restricts every item's loading $\lambda$ and threshold $\tau$ parameters to have the same value in the two cohorts. It assumes that the items in our scale have the same relationship with latent skills across the two cohorts. In other words, items have the same salience, or informational content relative to skills. If this model fits as well as the configural model, we can be confident that the socioemotional skills of children in the two cohorts can be placed on the same scale, and their \emph{variances} can be compared. To see why, consider equation \eqref{eq:propens}. If the loading matrix $\bm{\Lambda}$ is the same across cohorts, any difference in latent skills $\Delta\bm{\theta}$ will correspond to the same difference in latent propensities $\Delta\bm{X}^*$. Equality of thresholds $\tau$ ensures that propensities $\bm{X}^*$ map into observed items $\bm{X}$ in the same way.

A \emph{loading, threshold, and intercept invariance} model additionally restricts every item's intercept $\nu$ across cohorts. A good relative fit of this model indicates that socioemotional skills can be compared across cohorts in terms of their \emph{means} as well. To see why, consider the following. Since the $\lambda$ and $\nu$ parameters are the same across cohorts, a child in the BCS cohort with a given level of latent skills $\bar{\bm{\theta}}$ will have the same expected latent item propensities $\bm{X}^*$ as a child with the same skills in the MCS cohort. Again, equality of thresholds $\tau$ fixes the mapping between $\bm{X}^*$ and $\bm{X}$.\footnote{We recognise that simultaneous invariance of \emph{all} items is not the minimum requirement for comparability. In theory, the availability of just one invariant item (known as `anchor') would suffice to fix the scale and location of the system. However, partial invariance approaches are hard to implement in practice. Its validity hinges on selecting one (or more) truly invariant anchor, which is challenging on an a priori basis. The full procedure, restricting all parameters of a certain type across groups, does not identify which items are at the source of the invariance. Algorithms have been proposed to deal with this issue \citep{Yoon2007,Cheung2012}, however there are still doubts on their robustness and their applicability to the categorical case \citep{Vandenberg2016}.}

We estimate the sequence of models detailed in \autoref{tab:invparam} by Weighted Least Squares.\footnote{Parameters are estimated by mean- and variance-adjusted weighted least squares (WLSMV) -- see \citet{Muthen1997}; estimation starts from the items' polychoric correlation matrix, uses diagonally weighted least squares (DWLS), and exploits the full weight matrix to compute robust standard errors and test statistics. Robust WLS has proved in simulation studies to be moderately robust to small violations of the normality assumption in the latent underlying measures \citep{Flora2004}, and generally outperforms maximum likelihood in large samples \citep{Beauducel2006,Li2016}. All estimates are computed using the \texttt{lavaan} package (version 0.6-2) in \textbf{\textsf{R}} \citep{Rosseel2012}.}
For the purposes of the analysis, we define groups $c$ as cohort-gender cells, with the reference group being males in the BCS cohort. We then compare the fit of each model against the configural model.

Comparison of $\chi^2$ values across models is a common likelihood-based strategy. However, tests based on $\Delta \chi^2$ are known to display high Type I error rates with large sample size and more complex models such as our own \citep{Sass2014}. In fact, for all invariance levels in our applications a chi-squared difference would point to a lack of measurement invariance. The use of approximate fit indices (AFIs) is therefore recommended alongside $\chi^2$. These indices do not have a known sampling distribution, thus making it necessary to rely on rules of thumb to assess what level of \textDelta AFI indicates invariance. Nevertheless, AFIs are widely used in empirical practice to assess model fit.\footnote{The root mean squared error of approximation (RMSE) and the Tucker-Lewis index (TLI) are traditionally the most used AFIs in empirical practice. Simulation evidence by \citet{Cheung2002} shows that these indices can show correlation between overall and relative fit, and suggest relying on additional indices, such as the comparative fit index (CFI, \citealp{Bentler1990}), McDonald non-centrality index (MFI, \citealp{McDonald1989}), and Gamma-hat index \citep{Steiger1989} for the case of ordered measures. Commonly accepted thresholds for rejection are $\Delta CFI < - 0.01$, $\Delta MFI < - 0.02$, and $\Delta Gamma hat < - 0.001$. \citet{Meade2008}, using the results from a simulation study, suggests stricter thresholds that should apply in a variety of conditions. For CFI, a single cutoff value of $.002$ is proposed, while cutoffs for MFI depend on the problem's characteristics; in our case (2 factors, 11 items), they suggest $.0066$. \citet{Sass2014} however cast some doubts of the generalisability of these cutoffs to WLSMV estimators.}

The fit of each model is compared in Panel A of \autoref{tab:fit}. The model with restricted thresholds and loadings exhibits a comparable fit to the configural model, according to all the AFIs. Invariance of loadings and thresholds across cohorts implies that items in our scale are equally salient in their informational content, and that the latent propensities have equal mapping into the observed items. However, further restricting intercepts results in a model where invariance is rejected across the board.\footnote{We do not present fit results for the threshold-only invariance model, as it is statistically equivalent to the configural model and thus its fit is mathematically the same -- see Table 3 in \citealp{Wu2016a}. The ages at which socio-emotional skills are observed varies slightly between BCS and MCS, due to different sampling and fieldwork schedules. In the MCS cohort, the age distribution has significantly higher variance. In Panel B of \autoref{tab:fit}, we restrict the sample to 59 to 61 months, where the overlap between BCS and MCS is maximised. In Panels C and D, we restrict to male and female children respectively. In all these cases, invariance of thresholds and loadings is achieved, but invariance of intercepts is rejected. We can thus rule out that the lack of intercept invariance comes from differences in ages or invariance across child gender.} In other words, intercept parameters in our model ($\nu$) are estimated to be different between maternal reports in the British and Millennium Cohort Studies. This means that, for a given level of latent skills, mothers in MCS tend to assess behaviours differently from mothers in BCS. Thus, cohort differences in scores on our scale cannot be unequivocally interpreted as differences in the underlying skills, since they might also reflect differences in reporting.

This is an important finding, which has to our knowledge never been acknowledged in this literature. How can this lack of comparability be explained?  A possible interpretation is connected with secular evolution of social and cultural norms about child behaviours. For example, commonly held views of what constitutes a restless, distracted, or unhappy child might have changed between 1975 and 2005/6.\footnote{Calibrating the Rutter and SDQ using a contemporary sample of children cannot rule out this issue. For example, \citet{Collishaw2004} administered both Rutter and SDQ items to parents of a small sample of adolescents in London. They use the mapping between the two questionnaires to impute Rutter scores for mothers who answered the SDQ. This can correct for contemporaneous reporting differences between questionnaires, but cannot tackle reporting differences between samples collected at different times in history.}

To summarise, our measurement invariance analysis shows partial comparability of socioemotional skills across cohorts. In particular, the variance of skills can be compared across cohorts, but mean cohort differences do not necessarily reflect differences in skills. We can use scores from our scale to compare children within the same cohort, but not across cohorts. However, we can also compare within-cohort differences between groups of children, across cohorts. As an example, consider two groups of children A and B in the BCS cohort, and two groups of children C and D in the MCS. We cannot compare the mean level of skills between groups A and C, but we can compare the mean difference between groups A and B with the mean difference between groups C and D. This is the approach we take for the rest of the paper. Refraining from direct cross-cohort comparisons, we interpreting significance and magnitude of within-cohort differences across the cohorts.


