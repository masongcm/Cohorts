
\section{Data \label{sec:data}}

We use information from two nationally representative longitudinal studies in the UK, which follow the lives of children born approximately 30 years apart: the British Cohort Study (BCS) and the Millennium Cohort Study (MCS). The BCS includes all individuals born in Great Britain in a single week in 1970. The cohort members' families -- and subsequently the members themselves -- were surveyed on multiple occasions. For this paper we augment the information collected at the five-year survey with data from birth, adolescence (16), and adulthood (30, 38, 42). The MCS follows individuals born in the UK between September 2000 and January 2002. We use the first survey -- carried out at 9 months of age -- and the sweeps at around 5 and 14 years of age.\footnote{All data is publicly available at the UK Data Service \citep{Chamberlain2013,Butler2016a,Butler2016b,Butler2017,UniversityOfLondon.InstituteOfEducation.CentreForLongitudinalStudies2016b,UniversityOfLondon.InstituteOfEducation.CentreForLongitudinalStudies2016,UniversityOfLondon.InstituteOfEducation.CentreForLongitudinalStudies2016a,UniversityOfLondon.InstituteOfEducation.CentreForLongitudinalStudies2017b,UniversityOfLondon.InstituteOfEducation.CentreForLongitudinalStudies2017,UniversityOfLondon.InstituteOfEducation.CentreForLongitudinalStudies2017c}.}

Our main focus is on socio-emotional skills of children around age five. We take advantage of the longitudinal nature of the cohorts by merging information from surveys before and after age five. From the birth survey, we include information on gestational age and weight at birth, previous stillbirths, parity, maternal smoking in pregnancy, maternal age, height, and marital status. From the five year survey, we extract maternal education, employment status, and the father's occupation. All the above variables are transformed or recoded to maximise comparability between the two studies. Furthermore, we add some adolescent outcomes such as smoking and BMI, with the caveat that these are surveyed at different ages -- 16 in BCS and 14 in MCS. Finally, for the 1970 cohort we also include measures of adult educational attainment, BMI, and income. Variable definitions are available in \autoref{tab:harmvar}.

Ideally, we would compare socio-emotional skills alongside cognitive skills. However, the cognitive tests administered to each cohort have no overlap, even at the item level. We thus use the available cognitive tests in each cohort to estimate simple confirmatory factor models with a single latent dimension, separately by cohort (see \autoref{tab:harmvar} for the tests used). Unlike the other indicators in our analysis, cognitive skills are thus not comparable across cohorts. 

Another complication arises from the fact that, differently from the British Cohort Study, the Millennium Cohort Study has a stratified design. It oversamples children living in administrative areas characterised by higher socioeconomic deprivation and larger ethnic minority population \citep{Plewis2007}. We rebalance the MCS sample to make it nationally representative by excluding from the analysis a fraction of observations from the oversampled areas, proportionally to their sampling probability.\footnote{See Table 5.5 in \citet{Plewis2007}. This choice is mainly driven by software limitations. The \texttt{lavaan} package in \textbf{\textsf{R}} \citep{Rosseel2012} is the most suitable tool for our invariance analysis, but it does not allow to use weights when outcomes are categorical, as it is the case for the socio-emotional measurements.} Finally, we also restrict our sample to individuals born in England and to cases where there is complete information on socio-emotional skills at five years of age. The final sample contains 9,545 individuals from the British Cohort Study, and 5,572 from the Millennium Cohort Study. Summary statistics for the full and estimation samples are displayed in \autoref{tab:sumstats}. After the rebalancing step, the MCS estimation sample closely mirrors the full sample in terms of average observable characteristics, thus preserving representativeness.

\section{Dimensions of  socio-emotional skills \label{sec:methods}}

Child socio-emotional skills are an unobservable and difficult to measure construct. Over recent years, the measurement of such skills has evolved and, over time, different measures have been used.  As we discuss below, this makes the comparison  of socio-emotional skills across different groups, assessed with different tools, difficult.

A common approach to infer a child's socio-emotional development is based on behavioural screening scales. As part of these tools, mothers (or teachers) indicate whether their children exhibit a series of behaviours -- the \emph{items} of the scale. In the British and Millennium Cohort Studies, two different scales were employed. In the BCS, the Rutter A Scale was used \citep{Rutter1970} while in the MCS mothers were  administered the Strengths and Difficulties Questionnaire (SDQ, \citealp{Goodman1994,Goodman1997}). The SDQ was created as an update to the Rutter scale. It encompasses more recent advances in child psychopathology, and emphasises positive traits alongside undesirable ones \citep{Stone2010}. \citet{Goodman1997} administered both scales to a sample of children, and showed that the scores are highly correlated, and the two measures do not differ in their discriminatory ability. The Rutter and SDQ scales are reproduced in \autoref{tab:scales}; they have 23 and 25 items each, respectively. In the child psychiatry and psychology literatures, the Rutter and SDQ scales are regarded as measures of behavioural problems and mental health. However, in our analysis we follow the economics literature, and - after having recoded them accordingly - we interpret them as measures of positive child development \citep{Goodman2011}.

While the Rutter and SDQ scales are similar in their components (since the latter was developed from the former, see \citet{Goodman1994}), there is no a priori reason to expect them to be directly comparable. First, the overlap of behaviours described in the two scales is only partial, given that - by design - the SDQ includes also strengths, in addition to weaknesses. Second, the wording of each item is slightly different, both in the description and in the options that can be selected as answers. Third, the different ordering of the items within each scale might lead to order effects. Fourth, and no less importantly, the interpretation of each behaviour by respondents living 30 years apart (1975 vs 2006) might differ due to a host of evolving societal norms. Nonetheless, the level of comparibility of the two scales is higher than that of other scales used in comparative work in the literature reviewed in section \ref{sec:lit}.

As our goal is to compare socio-emotional skills across the two cohorts, we construct a new scale by retaining the items that are worded in a similar way across the two original Rutter and SDQ scales, and making some slight coding adjustments to maximise comparability. In what follows, we will consider the included items to be the same \emph{measure} in the two cohorts. The wording of the items we will be using in the analysis is presented in \autoref{tab:scalecomp}: we retain 13 items for the BCS (two of them are grouped) and 11 for the MCS with high degree of comparability. We exclude from the analysis items that were completely different between the two questionnaires to maximise comparability between the two cohorts, as it is standard good practice in the psychometric literature (see for example \citet{Kern2014}).\footnote{Of course, we could have included them in the factor analysis and treated them as missing in the cohort where they were not administered.} More details on the derivation of the scale are available in Appendix \autoref{asec:scales}.

Item-level prevalence by cohort and gender is in \autoref{tab:meantab}. We see that, in general, there are more similarities across genders within the same cohort, than across cohorts. For the majority of items, there is a lower prevalence of problematic behaviours in the MCS than in the BCS; however, four items (distracted, tantrums, fearful, aches) show a higher prevalence in 2006 than in 1975. Regardless, a simple cross-cohort comparison of item-level prevalence is misleading because of changing perceptions and norms about what constitutes problematic behaviour in children. The analysis in section \ref{sec:measinv} tackles this issue.

In the remainder of this section, we analyse the properties of the new scale. Following a common approach, we proceed in two steps. First, we carry out an \emph{exploratory} step, where we study the factor structure of our scale. The aim of this step is to examine the correlation between observed measures in a data-driven way, imposing the least possible assumptions. Here, we establish how many latent dimensions of socio-emotional skills the scale is capturing, and which items of our scale are measuring which dimension. As a second step, we set up a \emph{confirmatory} factor model. This model fixes the number of latent dimensions, and imposes a dedicated measurement structure, based on the insights obtained in the exploratory step. This is the model to which we apply the measurement invariance analysis of \autoref{sec:measinv}.

\subsection{Exploratory analysis \label{sec:ea}}

The original Rutter scale, used in the BCS cohort, distinguishes behaviours into two subscales: \emph{anti-social} and \emph{neurotic} \citep{Rutter1970}. This two-factor conceptualisation has been validated using data from multiple contexts, and the latent dimensions have been broadly identified as externalising and internalising behaviour problems.\footnote{See for example \citet{Fowler1979,Venables1983,Tremblay1987,Berglund1999,Klein2009}. However, in some cases a three-factor structure was found to better fit the data, with the externalising factor separating into two factors seemingly capturing aggressive and hyperactive behaviours \citep{Behar1974,McGee1985}.} The Strength and Difficulties Questionnaire, used in the MCS cohort, was instead conceived to have five subscales of five items each. The five subscales are: \emph{hyperactivity}, \emph{emotional symptoms}, \emph{conduct problems}, \emph{peer problems}, and \emph{prosocial}. This five-factor structure has been validated in many contexts \citep{Stone2010}; lower-dimensional structures have been also suggested \citep{Dickey2004}. Recent research has shown that there are some benefits to using broader subscales that correspond to the externalising and internalising factors in Rutter, especially in low-risk or general population samples \citep{Goodman2010}. Indeed, the internalising and externalising dimensions were introduced in psychology by \citet{Achenbach1966}, who showed that they are the two main factors underlying a wide range of psychological measures; as noted in \citet{Achenbach2016}, more than 75,000 articles have been published on internalizing and externalizing problems.

We use exploratory factor analysis (EFA) to assess the factor structure of our new scale, composed of 11 items of the Rutter scale in the BCS and the corresponding items of the SDQ in the MCS.\footnote{Factor-analytic methods have long been used in psychology, and in recent years they have become increasingly popular in economics, especially to meaningfully aggregate high-dimensional items measuring different aspects of common underlying dimensions of human development. The EFA is performed decomposing the polychoric correlation matrix of the items and using weighted least squares, and the solution is rescaled using oblique factor rotation (\emph{oblimin}). We use the \textbf{\textsf{R}} package \texttt{psych}, version 1.8.4 \citep{Revelle2018}.} We start by investigating the number of latent constructs that are captured by the scale, using different methods developed in the psychometric literature, and recently adopted by the economics literature. The results are displayed in \autoref{tab:numfac}. As pointed out in \citet{Conti2014}, there is relatively little agreement among procedures; this is the case especially for the Rutter items in the BCS data, where different methods suggest to retain between 1 and 3 factors, while most methods suggest to retain 2 factors for the SDQ items in the MCS. 

Given the test results, we perform a series of exploratory factor analyses, assuming a one-, two- or three-factor structure, respectively. The results for the 1-factor solution, reported in \autoref{tab:load_add}, show relatively similar loadings for both males and females across the two cohorts, of slightly bigger magnitude for the last four items in the MCS than in the BCS; thus, we retain the 1-factor solution for the measurement invariance analysis, in the first instance. The results for the 3-factor solution, instead, also reported in \autoref{tab:load_add}, show a less homogeneous picture:\footnote{We do not perform the EFA with 3 factors for males because this solution is never chosen by any test for the number of factors for the MCS, see \autoref{tab:numfac}.} while the magnitude of the loadings is relatively similar across the two cohorts for the first factor, items 3 and 5 only load on the second factor for the MCS, not for the BCS; more importantly, the EFA clearly shows that the third factor only loads on one single item (item number 9, ``solitary'') for both cohorts. Given that a one-item factor implies that the item perfectly proxies for the factor, we are not able to test for measurement invariance in this case. Hence, the 3-factor solution is not supported by our EFA results. Last, the two-factor EFA is shown in \autoref{tab:load} and delivers a neat and sensible separation between items: similarly-worded items load on the same factor across the two cohorts, and also the magnitude of the respective loadings (measuring the strength of the association between the item and the factor) is very similar. Following previous research, we name the first dimension \emph{Externalising skills} (EXT, indicating low scores on the items restless, squirmy/fidgety, fights/bullies, distracted, tantrums, and disobedient) and the second dimension \emph{Internalising skills} (INT, indicating low scores on the items worried, fearful, solitary, unhappy, and aches).\footnote{Internalising and externalising dimensions emerge from the exploratory step on our novel 11-item scale. Appendix \autoref{asec:fullefa} performs the same exploratory steps on the full set of Rutter items in BCS and SDQ items in MCS. It confirms that the items we select for our subscale have a broadly consistent covariance structure even when factor-analysed with the others in their original scales. Appendix \autoref{asec:no511} considers the robustness of our results to the exclusion of the items of the scale that perform most poorly.} 

\subsection{Factor model \label{sec:fm}}

After having studied the factor structure underlying the 11 common items in the previous section, we now specify a multiple-group factor analysis model to formally quantify the strength of the relationship between the observed items in our scale and the latent socio-emotional skills, and to test for invariance across cohorts. We specify two groups of children $c=\{BCS,MCS\}$, corresponding to the two cohorts. Each individual child is denoted by $j=1\dots N_c$, where $N_c$ is the number of children in cohort $c$. For each child $j$ in cohort $c$, we observe categorical items $X_{ijc}$ with $i=1,\dots,11$ corresponding to the eleven maternal reports in \autoref{tab:scalecomp}. Following the EFA results above, we specify two models: one in which we assume that each child is characterised by only one latent skills vector, and another in which we assume that each child is characterised by a latent bi-dimensional vector of externalising and internalising socio-emotional skills $\bm{\theta}_{jc} = (\theta_{jc}^{EXT},\theta_{jc}^{INT})$.

Children are assumed to have a latent continuous propensity $X^*_{ijc}$ for each item $i=1,\dots,I$. We model this propensity as a function of item- and cohort-specific intercepts $\nu_{ic}$ and loadings $\bm{\lambda}_{ic}$, and the child's latent skills $\bm{\theta}_{jc}$, plus an independent error component $u_{ijc}$. The propensity for each item can be written as follows:

$$X^*_{ijc} =\nu_{ic} + \bm{\lambda}_{ic}\bm{\theta}_{jc} + u_{ijc} \qquad \text{for} \quad i = 1,\dots,11$$

or more compactly:
\be\label{eq:propens}
\bm{X}^*_{jc} =\bm{\nu}_c + \bm{\Lambda}_{c}\bm{\theta}_{jc} + \bm{u}_{jc}
\ee

We make the common assumption of a dedicated (or congeneric) factor structure, where each measure is assumed to load on only one latent dimension \citep{Heckman2013,Conti2010a,Attanasio2018a}. We mirror the structure found in the exploratory factor analysis above, and assume that all items load on one factor for the 1-factor solution (\autoref{tab:load_add}), and that items 1-6 load exclusively on the externalising factor and items 7-11 on the internalising factor for the 2-factor solution (\autoref{tab:load}).\footnote{The dedicated factor structure in the two-factor case corresponds to a sparse loading matrix, i.e.: $$\bm{\Lambda}_{c} \coloneqq \begin{bmatrix} \lambda_{1c}, \dots, \lambda_{6c} & \bm{0} \\ \bm{0} & \lambda_{7c}, \dots, \lambda_{11c} \end{bmatrix}.$$} 

The discrete ordered nature of the observed measures $X_{ijc}$ is incorporated by introducing item- and cohort-specific threshold parameters $\tau_{ic}$ \citep{Muthen1984}. The observed measures as a function of the propensities $X^*$ can be then written as follows:
\be\label{eq:thresh}
X_{ijc} = s \qquad \text{if} \; \tau_{s,ic} \leq X^*_{ijc} < \tau_{s+1,ic} \qquad \text{for} \; s=0,1,2
\ee
with $\tau_{0,ic}=-\infty$ and $\tau_{3,ic}=+\infty$. Notice that we recode all ordered items to have higher values for \emph{better} behaviours, so that our latent vectors can be interpreted as favourable skills and not behavioural problems.\footnote{The model implies the following expression for the mean and covariance structure of the latent propensities:
$$\bm{\mu}_c=\bm{\nu}_c + \bm{\Lambda}_c\bm{\kappa}_c \qquad \text{and} \qquad \Sigma_c = \bm{\Lambda}_c \bm{\Phi}_c \bm{\Lambda}_c^\prime + \bm{\Psi}_c.$$
As per the traditional factor analysis approach, we impose a normal distribution on the latent skills and error terms. 
\be\label{eq:norm}
\bm{\theta}_{jc} \sim N(\bm{\kappa}_c, \bm{\Phi}_c) \qquad \text{and} \qquad \bm{u}_{jc} \sim N(\bm{0}, \bm{\Psi}_c).
\ee
Recent work has also used mixtures of normals for the latent factors distribution, e.g. \citet{Conti2010a}.}

\section{Measurement invariance \label{sec:measinv}}

\subsection{The configural model \label{sec:config}}

Measurement invariance analysis necessarily starts from a minimally restrictive model, denominated \emph{configural} model. This is a `minimum' identifiable model, in that it places the least possible restrictions on how parameters are allowed to vary across cohorts. The restrictions implied in \eqref{eq:propens} and \eqref{eq:thresh} are not sufficient to identify the parameters of the model: even with these assumptions, there are infinite equivalent parameterisations (or rotations) that deliver a minimally restrictive configural model. This is the well-known issue of factor indeterminacy, which arises due to the lack of natural units of measurement for the latent factors being assessed.

Further sets of restrictions are thus required to set the location and scale of the latent factors. Among the most straightforward and widely used parameterisations for the configural model are:
\bi[label=$\diamond$]
\item \underline{Delta parameterisation} {[WE{\textDelta}]} \citep{Wu2016a}

For all groups: $$\text{diag}(\bm{\Phi}) = \bm{I}, \quad \bm{\kappa}=\bm{0}, \quad \bm{\nu}=\bm{0}, \quad \text{and} \; \text{diag}(\bm{\Sigma})=\bm{I}.$$

\item \underline{Theta parameterisation} {[WE{\textTheta}]} \citep{Wu2016a}

For all groups: 
\be\label{eq:thetapar}
\text{diag}(\bm{\Phi}) = \bm{I}, \quad \bm{\kappa}=\bm{0}, \quad \bm{\nu}=\bm{0}, \quad \text{and} \; \text{diag}(\bm{\Psi})=\bm{I}.
\ee

\item \underline{Anchored parameterisation} {[MT]} \citep{Millsap2004}
  \bi
  \item For all groups, normalise a reference loading to 1 for each factor.
  \item Set invariant across groups one threshold per item (e.g. $\tau_{0,Ai} = \tau_{0,Bi}$ for two groups $A$ and $B$), and an additional threshold in the reference items above.
  \item In the first group: $\bm{\kappa_A}=\bm{0}$, $\text{diag}(\bm{\Sigma}_A)=\bm{I}$.
  \item Set all intercepts $\bm{\nu}$ to zero.
  \ei
\ei
The first two parameterisations (WE{\textDelta} and WE{\textTheta}) normalise the mean and variance of factors to the same constants in both groups, and they leave all loadings and thresholds to be freely estimated; they only differ in whether the additional required normalisation is imposed on the variances of the error terms ($\bm{\Psi}$) or on the diagonal of the covariance matrix of the measures ($\bm{\Sigma}$). The MT parameterisation instead proceeds by identifying parameters in one group first, and then imposing cross-group equality constraints to identify parameters in other groups \citep{Wu2016a}. Still, all of these parameterisations are statistically equivalent. The measurement invariance analysis in this paper is based on the Theta parameterisation (WE{\textTheta}), but results are independent on this choice. The restrictions in \eqref{eq:propens}, \eqref{eq:thresh}, and \eqref{eq:thetapar} define the so-called \emph{configural} model.

\subsection{Nested models \label{sec:nested}}

Any comparison between socio-emotional skills across the two cohorts requires that the measures at our disposal have the same relationship with the latent constructs of interest in both cohorts. In other words, the items in our new scale must measure socio-emotional skills in the same way in the BCS and MCS data. This property is denominated measurement invariance (MI) \citep{Vandenberg2000a,Putnick2016}.

In the framework of factor analysis, measurement invariance is a formally testable property. In this paper, we follow the recent identification methodology by \citet{Wu2016a}. The configural model defined above in section \ref{sec:config} serves as the starting point. Measurement invariance is then assessed by comparing the configural model to a series of hierarchically nested models. These models place increasing restrictions on the item parameters, constraining them to be equal across groups. Their fit is then compared to that of the configural model. Intuitively, if the additional cross-group restrictions have not significantly worsened model fit, one can conclude that a certain level of invariance is achieved. 

In the case where the available measures are continuous, MI analysis is straightforward \citep{vandeSchoot2012}. The hierarchy of the nested models usually proceeds by testing loadings first, and then intercepts (to establish \emph{metric} and \emph{scalar} invariance -- see \citealp{Vandenberg2000a}). Invariance of systems with categorical measures, such as the scale we examine in this paper, is less well understood. In particular, the lack of explicit location and scale in the measures introduces an additional set of parameters compared to the continuous case (thresholds $\tau$). This makes identification reliant on more stringent normalisations. A first comprehensive approach for categorical measures was proposed by \cite{Millsap2004}. New identification results in \cite{Wu2016a} indicate that, in the categorical case, invariance properties cannot be examined by simply restricting one set of parameters at a time. This is because the identification conditions used in the configural baseline model, while being minimally restrictive on their own, become binding once certain additional restrictions are imposed. In light of this, they propose models that identify structures of different invariance levels. They find that some restrictions cannot be tested alone against the configural model, because the models they generate are statistically equivalent. This is true of loading invariance, and also of threshold invariance in the case when the number of categories of each ordinal item is 3 or less. Furthermore, they suggest that comparison of both latent means and variances requires invariance in loadings, thresholds, and intercepts. A summary of the approach by \cite{Wu2016a} is available in \autoref{tab:invparam}.

Let's consider examples from our application. A \emph{loading and threshold invariance} model restricts every item's loading $\lambda$ and threshold $\tau$ parameters to have the same value in the two cohorts. It assumes that the items in our scale have the same relationship with latent skills across the two cohorts. In other words, items have the same salience, or informational content relative to skills. If this model fits as well as the configural model, we can be confident that the socio-emotional skills of children in the two cohorts can be placed on the same scale, and their \emph{variances} can be compared. To see why, consider equation \eqref{eq:propens}. If the loading matrix $\bm{\Lambda}$ is the same across cohorts, any difference in latent skills $\Delta\bm{\theta}$ will correspond to the same difference in latent propensities $\Delta\bm{X}^*$. Equality of thresholds $\tau$ ensures that propensities $\bm{X}^*$ map into observed items $\bm{X}$ in the same way.

A \emph{loading, threshold, and intercept invariance} model additionally restricts every item's intercept $\nu$ across cohorts. A good relative fit of this model indicates that socio-emotional skills can be compared across cohorts in terms of their \emph{means} as well. To see why, consider the following. Since the $\lambda$ and $\nu$ parameters are the same across cohorts, a child in the BCS cohort with a given level of latent skills $\bar{\bm{\theta}}$ will have the same expected latent item propensities $\bm{X}^*$ as a child with the same skills in the MCS cohort. Again, equality of thresholds $\tau$ fixes the mapping between $\bm{X}^*$ and $\bm{X}$.\footnote{We recognise that simultaneous invariance of \emph{all} items is not the minimum requirement for comparability. In theory, the availability of just one invariant item (known as `anchor') would suffice to fix the scale and location of the system. However, partial invariance approaches are hard to implement in practice. Its validity hinges on selecting one (or more) truly invariant anchor, which is challenging on an a priori basis. The full procedure, restricting all parameters of a certain type across groups, does not identify which items are at the source of the invariance. Algorithms have been proposed to deal with this issue \citep{Yoon2007,Cheung2012}, however there are still doubts on their robustness and their applicability to the categorical case \citep{Vandenberg2016}.}

We estimate the sequence of models detailed in \autoref{tab:invparam} by mean- and variance-adjusted weighted least squares (WLSMV) -- see \citet{Muthen1997}; estimation starts from the items' polychoric correlation matrix, uses diagonally weighted least squares (DWLS), and exploits the full weight matrix to compute robust standard errors and test statistics. Robust WLS has proved in simulation studies to be moderately robust to small violations of the normality assumption in the latent underlying measures \citep{Flora2004}, and generally outperforms maximum likelihood in large samples \citep{Beauducel2006,Li2016}.\footnote{All estimates are computed using the \texttt{lavaan} package (version 0.6-2) in \textbf{\textsf{R}} \citep{Rosseel2012}.}
For the purposes of the analysis, we define groups $c$ as cohort-gender cells, with the reference group being males in the BCS cohort. We then compare the fit of each model against the configural model.

\subsection{Measurement invariance results \label{sec:mires}}

Comparison of $\chi^2$ values across models is a common likelihood-based strategy. However, tests based on $\Delta \chi^2$ are known to display high Type I error rates with large sample size and more complex models such as our own \citep{Sass2014}. In fact, for all invariance levels in our applications a chi-squared difference would point to a lack of measurement invariance. The use of approximate fit indices (AFIs) is therefore recommended alongside $\chi^2$. While these indices successfully adjust for model complexity \citep{Cheung2002}, they do not have a known sampling distribution. This makes it necessary to rely on simulation studies, which derive rules of thumb indicating what level of \textDelta AFI is compatible with invariance.

Again, just like in the broader context of measurement invariance, most evidence regarding the performance of AFIs pertains to scenarios with continuous measures. The root mean squared error of approximation (RMSE) and the Tucker-Lewis index (TLI) are traditionally the most used AFIs in empirical practice. Simulation evidence by \citet{Cheung2002} shows that these indices can show correlation between overall and relative fit, and suggest relying on additional indices, such as the comparative fit index (CFI, \citealp{Bentler1990}), McDonald non-centrality index (MFI, \citealp{McDonald1989}), and Gamma-hat index \citep{Steiger1989}. Subsequent simulation studies -- e.g. \citet{Chen2007} and \citet{Meade2008} -- have updated these thresholds for the continuous case. In particular, \citet{Chen2007} shows in two Monte Carlo studies that the standardised root mean square residual (RMSR) is more sensitive to lack of invariance in factor loadings than in intercepts or residual variances, while the CFI and RMSEA are equally sensitive to all three types of lack of invariance; he suggests the following thresholds for rejecting measurement invariance: \textDelta RMSE $> .015$, \textDelta CFI $< - .010$, \textDelta RMSR $> .010$. 

However, it is not advisable to directly extrapolate rules of thumb derived from simulations with continuous measures to the categorical case \citep{Lubke2004}. Recent studies have advanced the simulation-based evidence on the performance of AFIs in measurement invariance analysis with categorical measures. \citet{Sass2014} find that the cutoffs from \citet{Chen2007} might not generalise well to problems estimated by WLSMV, but this is mostly confined to smaller sample sizes and detection of small degrees of non-invariance. More recently, \citet{Rutkowski2017} find that a \textDelta RMSE threshold of $.010$ is appropriate for testing equality of slopes and thresholds when the sample size is large, like in our case. 

In any case, we present a range of fit indices to provide a more complete assessment of measurement invariance. We present the measurement invariance results for the 1-factor model in \autoref{tab:fit_1f}, and those for the 2-factor model in \autoref{tab:fit}. First, by comparing the fit of each nested model across the 1-factor and the 2-factor models, it is clear that the 1-factor model fits the data significantly worse than the 2-factor model, according to all the criteria considered.\footnote{It is  worth noting that threshold and loading invariance only can be established also in the 1-factor case, i.e. intercept invariance is never achieved.} Hence, in our analysis since now on, we adopt the two-factor solution, which is also consistent with the child psychology literature cited above: as mentioned above, we name the two factors externalizing and internalizing skills. We now examine the measurement invariance properties of our chosen two-factor solution in greater details. Looking at Panel A of \autoref{tab:fit}, we see that the overall fit of the configural model for the chosen 2-factor solution is satisfactory according to all indices, with CFI around $.95$ and RMSE just above $.05$. As expected, given our large sample size, $\chi^2$-based tests reject measurement invariance at all levels. The model with restricted thresholds and loadings exhibits a comparable fit to the configural model, according to all the AFIs. In particular, the \textDelta AFIs fall within the ranges suggested in \citet{Chen2007}, \citet{Rutkowski2017} and \citet{Svetina2017}; see also \citet{Svetina2019} for a review of updated guidelines for measurement invariance. Invariance of loadings and thresholds across cohorts implies that the items in our scale are equally salient in their informational content, and that the latent propensities have equal mapping into the observed items. 

However, further restricting intercepts results in a model where invariance is rejected across the board. In other words, intercept parameters in our model ($\nu$) are estimated to be different between maternal reports in the British and Millennium Cohort Studies. This means that, for a given level of latent skills, mothers in MCS tend to assess behaviours differently from mothers in BCS. Thus, cohort differences in scores on our scale cannot be unequivocally interpreted as differences in the underlying skills, since they might also reflect differences in reporting.\footnote{We do not present fit results for the threshold-only invariance model, as it is statistically equivalent to the configural model and thus its fit is mathematically the same -- see Table 3 in \citealp{Wu2016a}. The ages at which socio-emotional skills are observed varies slightly between BCS and MCS, due to different sampling and fieldwork schedules. In the MCS cohort, the age distribution has significantly higher variance. In Panel B of \autoref{tab:fit}, we restrict the sample to 59 to 61 months, where the overlap between BCS and MCS is maximised. In Panel C, we repeat the analysis with the full sample, but excluding the poorest-performing items (5 and 11) -- see Appendix \autoref{asec:no511} for details. In Panels A and B of \autoref{tab:fit_gend}, we restrict to male and female children respectively. In all these cases, invariance of thresholds and loadings is confirmed, but invariance of intercepts is rejected. We can thus rule out that the lack of intercept invariance comes from differences in ages or invariance across child gender.}

This is an important finding, which has to our knowledge never been acknowledged in the economic literature. How can this lack of comparability be explained?  A possible interpretation is connected with secular evolution of social and cultural norms about child behaviours. For example, commonly held views of what constitutes a restless, distracted, or unhappy child might have changed between 1975 and 2006.\footnote{Calibrating the Rutter and SDQ using a contemporary sample of children cannot rule out this issue. For example, \citet{Collishaw2004} administered both Rutter and SDQ items to parents of a small sample of adolescents in London. They use the mapping between the two questionnaires to impute Rutter scores for mothers who answered the SDQ. This can correct for contemporaneous reporting differences between questionnaires, but cannot tackle reporting differences between samples collected at different times in history.}

To summarise, our measurement invariance analysis shows partial comparability of socio-emotional skills across cohorts. In particular, the variance of skills can be compared across cohorts, but mean cohort differences do not necessarily reflect differences in skills. We can use scores from our scale to compare children within the same cohort-gender group, but not across cohorts. However, we can also compare within-cohort differences between groups of children, across cohorts. As an example, consider two groups of children A and B in the BCS cohort, and two groups of children C and D in the MCS. We cannot compare the mean level of skills between groups A and C, but we can compare the mean difference between groups A and B with the mean difference between groups C and D. This is the approach we take for the rest of the paper. Refraining from direct cross-cohort comparisons, we interpreting significance and magnitude of within-cohort differences across the cohorts.
